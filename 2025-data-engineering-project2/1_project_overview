# Sparkify S3 to Redshift ETL Project

## Project Overview
Sparkify, a music streaming startup, is migrating its data processes to the cloud. The goal is to build an ETL pipeline that extracts data from AWS S3, stages it in Amazon Redshift, and transforms it into a star schema for analytical queries.

## Data Sources
- **S3 Buckets:**
  - `song_data`: JSON metadata about songs.
  - `log_data`: JSON logs of user activity.
  - `log_json_path`: JSON format configuration for parsing.

## ETL Process
1. **Extract**: Load raw data from S3 into Redshift staging tables.
2. **Transform**: Clean and structure data into facts and dimensions.
3. **Load**: Insert processed data into an analytical star schema.

## Data Warehouse Schema (Star Schema)
- **Fact Table:** `songplays` (stores song play events)
- **Dimension Tables:**
  - `users` (user information)
  - `songs` (song details)
  - `artists` (artist details)
  - `time` (timestamp breakdown)

## Redshift Staging Tables
- `staging_songs` (raw song data)
- `staging_logs` (raw log data)

## Technologies Used
- **AWS S3**: Data storage
- **AWS Redshift**: Cloud-based data warehouse
- **Python**: ETL scripting
- **SQL**: Data transformation and querying

## Key Considerations
- Bulk data loading using `COPY` for efficiency.
- Role-based access control for security.
- Optimized schema design for analytical performance.

## Goal
Enable Sparkify's analytics team to gain insights on user behavior and song preferences efficiently using a scalable cloud-based data warehouse.


# Project Summary

## Project Datasets

This project utilizes two datasets stored in Amazon S3:
- **Song Data**: `s3://udacity-dend/song_data`
- **Log Data**: `s3://udacity-dend/log_data`
- **Metadata** (for parsing log data): `s3://udacity-dend/log_json_path.json`

The datasets are structured in JSON format:
- **Song Dataset**: Extracted from the Million Song Dataset, containing song metadata.
- **Log Dataset**: Simulated event logs from a music streaming app.

## Schema for Song Play Analysis

A star schema is used for optimizing queries:

### **Fact Table**
- **songplays** - records of song plays
  - `songplay_id`, `start_time`, `user_id`, `level`, `song_id`, `artist_id`, `session_id`, `location`, `user_agent`

### **Dimension Tables**
- **users** - user details
  - `user_id`, `first_name`, `last_name`, `gender`, `level`
- **songs** - song details
  - `song_id`, `title`, `artist_id`, `year`, `duration`
- **artists** - artist details
  - `artist_id`, `name`, `location`, `latitude`, `longitude`
- **time** - timestamp details
  - `start_time`, `hour`, `day`, `week`, `month`, `year`, `weekday`

## Project Steps

### **1. Create Table Schemas**
- Define table schemas in `sql_queries.py`
- Implement table creation logic in `create_table.py`
- Configure Redshift cluster and IAM roles

### **2. Build ETL Pipeline**
- Extract data from S3 and load into Redshift staging tables (`etl.py`)
- Transform and load data into analytics tables
- Validate data with test queries

### **3. Document Process**
- Explain database design and ETL process in `README.md`
- Provide example queries for analytical insights

## Project Files
- `create_table.py` - Defines and creates database tables
- `etl.py` - Extracts, transforms, and loads data into Redshift
- `sql_queries.py` - SQL statements for table creation and data loading
- `README.md` - Documentation of project details

## Notes
- Redshift does not support `SERIAL`; use `IDENTITY(0,1)` instead.
- Do not include AWS credentials in the code.

## Next Steps
- Run ETL pipeline and validate results
- Optimize query performance
- Expand dataset integration

