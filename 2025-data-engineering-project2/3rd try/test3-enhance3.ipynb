{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify S3 to Redshift ETL Project\n",
    "\n",
    "## Project Overview\n",
    "Sparkify, a music streaming startup, is migrating its data processes to the cloud. The goal is to build an ETL pipeline that extracts data from AWS S3, stages it in Amazon Redshift, and transforms it into a star schema for analytical queries.\n",
    "\n",
    "## Data Sources\n",
    "- **S3 Buckets:**\n",
    "  - `song_data`: JSON metadata about songs.\n",
    "  - `log_data`: JSON logs of user activity.\n",
    "  - `log_json_path`: JSON format configuration for parsing.\n",
    "\n",
    "## ETL Process\n",
    "1. **Extract**: Load raw data from S3 into Redshift staging tables.\n",
    "2. **Transform**: Clean and structure data into facts and dimensions.\n",
    "3. **Load**: Insert processed data into an analytical star schema.\n",
    "\n",
    "## Data Warehouse Schema (Star Schema)\n",
    "- **Fact Table:** `songplays` (stores song play events)\n",
    "- **Dimension Tables:**\n",
    "  - `users` (user information)\n",
    "  - `songs` (song details)\n",
    "  - `artists` (artist details)\n",
    "  - `time` (timestamp breakdown)\n",
    "\n",
    "## Redshift Staging Tables\n",
    "- `staging_songs` (raw song data)\n",
    "- `staging_logs` (raw log data)\n",
    "\n",
    "## Technologies Used\n",
    "- **AWS S3**: Data storage\n",
    "- **AWS Redshift**: Cloud-based data warehouse\n",
    "- **Python**: ETL scripting\n",
    "- **SQL**: Data transformation and querying\n",
    "\n",
    "## Key Considerations\n",
    "- Bulk data loading using `COPY` for efficiency.\n",
    "- Role-based access control for security.\n",
    "- Optimized schema design for analytical performance.\n",
    "\n",
    "## Goal\n",
    "Enable Sparkify's analytics team to gain insights on user behavior and song preferences efficiently using a scalable cloud-based data warehouse.\n",
    "\n",
    "\n",
    "# Project Summary\n",
    "\n",
    "## Project Datasets\n",
    "\n",
    "This project utilizes two datasets stored in Amazon S3:\n",
    "- **Song Data**: `s3://udacity-dend/song_data`\n",
    "- **Log Data**: `s3://udacity-dend/log_data`\n",
    "- **Metadata** (for parsing log data): `s3://udacity-dend/log_json_path.json`\n",
    "\n",
    "The datasets are structured in JSON format:\n",
    "- **Song Dataset**: Extracted from the Million Song Dataset, containing song metadata.\n",
    "- **Log Dataset**: Simulated event logs from a music streaming app.\n",
    "\n",
    "## Schema for Song Play Analysis\n",
    "\n",
    "A star schema is used for optimizing queries:\n",
    "\n",
    "### **Fact Table**\n",
    "- **songplays** - records of song plays\n",
    "  - `songplay_id`, `start_time`, `user_id`, `level`, `song_id`, `artist_id`, `session_id`, `location`, `user_agent`\n",
    "\n",
    "### **Dimension Tables**\n",
    "- **users** - user details\n",
    "  - `user_id`, `first_name`, `last_name`, `gender`, `level`\n",
    "- **songs** - song details\n",
    "  - `song_id`, `title`, `artist_id`, `year`, `duration`\n",
    "- **artists** - artist details\n",
    "  - `artist_id`, `name`, `location`, `latitude`, `longitude`\n",
    "- **time** - timestamp details\n",
    "  - `start_time`, `hour`, `day`, `week`, `month`, `year`, `weekday`\n",
    "\n",
    "## Project Steps\n",
    "\n",
    "### **1. Create Table Schemas**\n",
    "- Define table schemas in `sql_queries.py`\n",
    "- Implement table creation logic in `create_table.py`\n",
    "- Configure Redshift cluster and IAM roles\n",
    "\n",
    "### **2. Build ETL Pipeline**\n",
    "- Extract data from S3 and load into Redshift staging tables (`etl.py`)\n",
    "- Transform and load data into analytics tables\n",
    "- Validate data with test queries\n",
    "\n",
    "### **3. Document Process**\n",
    "- Explain database design and ETL process in `README.md`\n",
    "- Provide example queries for analytical insights\n",
    "\n",
    "## Project Files\n",
    "- `create_table.py` - Defines and creates database tables\n",
    "- `etl.py` - Extracts, transforms, and loads data into Redshift\n",
    "- `sql_queries.py` - SQL statements for table creation and data loading\n",
    "- `README.md` - Documentation of project details\n",
    "\n",
    "## Notes\n",
    "- Redshift does not support `SERIAL`; use `IDENTITY(0,1)` instead.\n",
    "- Do not include AWS credentials in the code.\n",
    "\n",
    "## Next Steps\n",
    "- Run ETL pipeline and validate results\n",
    "- Optimize query performance\n",
    "- Expand dataset integration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Creating Redshift Cluster\n",
    "\n",
    "Tihs part we can look at code from homework \"L3 exercise 2 - IaC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dwh-3.cfg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "# CONFIG\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh-3.cfg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events;\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs;\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_events (\n",
    "    artist VARCHAR,\n",
    "    auth VARCHAR,\n",
    "    firstName VARCHAR,\n",
    "    gender VARCHAR,\n",
    "    itemInSession INTEGER,\n",
    "    lastName VARCHAR,\n",
    "    length FLOAT,\n",
    "    level VARCHAR,\n",
    "    location VARCHAR,\n",
    "    method VARCHAR,\n",
    "    page VARCHAR,\n",
    "    registration FLOAT,\n",
    "    sessionId INTEGER,\n",
    "    song VARCHAR,\n",
    "    status INTEGER,\n",
    "    ts BIGINT,\n",
    "    userAgent VARCHAR,\n",
    "    userId INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_songs (\n",
    "    num_songs INTEGER,\n",
    "    artist_id VARCHAR,\n",
    "    artist_latitude FLOAT,\n",
    "    artist_longitude FLOAT,\n",
    "    artist_location VARCHAR,\n",
    "    artist_name VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    title VARCHAR,\n",
    "    duration FLOAT,\n",
    "    year INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "songplay_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplays (\n",
    "    songplay_id INTEGER IDENTITY(0,1) PRIMARY KEY,\n",
    "    start_time TIMESTAMP NOT NULL,\n",
    "    user_id INTEGER NOT NULL,\n",
    "    level VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    artist_id VARCHAR,\n",
    "    session_id INTEGER,\n",
    "    location VARCHAR,\n",
    "    user_agent VARCHAR\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY (start_time)\n",
    "SORTKEY (start_time);                           \n",
    "\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id INTEGER PRIMARY KEY,\n",
    "    first_name VARCHAR,\n",
    "    last_name VARCHAR,\n",
    "    gender VARCHAR,\n",
    "    level VARCHAR\n",
    ")\n",
    "SORTKEY (user_Id);                     \n",
    "\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songs (\n",
    "    song_id VARCHAR PRIMARY KEY,\n",
    "    title VARCHAR,\n",
    "    artist_id VARCHAR,\n",
    "    year INTEGER,\n",
    "    duration FLOAT\n",
    ")\n",
    "SORTKEY (song_id);                     \n",
    "\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artists (\n",
    "    artist_id VARCHAR PRIMARY KEY,\n",
    "    name VARCHAR,\n",
    "    location VARCHAR,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT\n",
    ")\n",
    "SORTKEY (artist_id);                       \n",
    "\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS time (\n",
    "    start_time TIMESTAMP PRIMARY KEY,\n",
    "    hour INTEGER,\n",
    "    day INTEGER,\n",
    "    week INTEGER,\n",
    "    month INTEGER,\n",
    "    year INTEGER,\n",
    "    weekday INTEGER\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY (start_time)\n",
    "SORTKEY (start_time);   \n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "create_table_queries = [staging_events_table_create, \n",
    "                        staging_songs_table_create, \n",
    "                        songplay_table_create, \n",
    "                        user_table_create, \n",
    "                        song_table_create, \n",
    "                        artist_table_create, \n",
    "                        time_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, \n",
    "                      staging_songs_table_drop, \n",
    "                      songplay_table_drop, \n",
    "                      user_table_drop, \n",
    "                      song_table_drop, \n",
    "                      artist_table_drop, \n",
    "                      time_table_drop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully\n",
      "Tables dropped successfully\n",
      "Tables created successfully\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    try:\n",
    "        for query in drop_table_queries:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print(\"Tables dropped successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Error dropping tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    try:\n",
    "        for query in create_table_queries:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print(\"Tables created successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "conn = None\n",
    "cur = None\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])  # Convert port to integer\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    print(\"Connected to Redshift successfully\")\n",
    "    \n",
    "    # Drop and create tables\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error connecting to Redshift: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "Show the tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table schemas in the database:\n",
      "    Schema           Table            Column                    Data Type  \\\n",
      "0   public         artists         artist_id            character varying   \n",
      "1   public         artists              name            character varying   \n",
      "2   public         artists          location            character varying   \n",
      "3   public         artists          latitude             double precision   \n",
      "4   public         artists         longitude             double precision   \n",
      "5   public       songplays       songplay_id                      integer   \n",
      "6   public       songplays        start_time  timestamp without time zone   \n",
      "7   public       songplays           user_id                      integer   \n",
      "8   public       songplays             level            character varying   \n",
      "9   public       songplays           song_id            character varying   \n",
      "10  public       songplays         artist_id            character varying   \n",
      "11  public       songplays        session_id                      integer   \n",
      "12  public       songplays          location            character varying   \n",
      "13  public       songplays        user_agent            character varying   \n",
      "14  public           songs           song_id            character varying   \n",
      "15  public           songs             title            character varying   \n",
      "16  public           songs         artist_id            character varying   \n",
      "17  public           songs              year                      integer   \n",
      "18  public           songs          duration             double precision   \n",
      "19  public  staging_events            artist            character varying   \n",
      "20  public  staging_events              auth            character varying   \n",
      "21  public  staging_events         firstname            character varying   \n",
      "22  public  staging_events            gender            character varying   \n",
      "23  public  staging_events     iteminsession                      integer   \n",
      "24  public  staging_events          lastname            character varying   \n",
      "25  public  staging_events            length             double precision   \n",
      "26  public  staging_events             level            character varying   \n",
      "27  public  staging_events          location            character varying   \n",
      "28  public  staging_events            method            character varying   \n",
      "29  public  staging_events              page            character varying   \n",
      "30  public  staging_events      registration             double precision   \n",
      "31  public  staging_events         sessionid                      integer   \n",
      "32  public  staging_events              song            character varying   \n",
      "33  public  staging_events            status                      integer   \n",
      "34  public  staging_events                ts                       bigint   \n",
      "35  public  staging_events         useragent            character varying   \n",
      "36  public  staging_events            userid                      integer   \n",
      "37  public   staging_songs         num_songs                      integer   \n",
      "38  public   staging_songs         artist_id            character varying   \n",
      "39  public   staging_songs   artist_latitude             double precision   \n",
      "40  public   staging_songs  artist_longitude             double precision   \n",
      "41  public   staging_songs   artist_location            character varying   \n",
      "42  public   staging_songs       artist_name            character varying   \n",
      "43  public   staging_songs           song_id            character varying   \n",
      "44  public   staging_songs             title            character varying   \n",
      "45  public   staging_songs          duration             double precision   \n",
      "46  public   staging_songs              year                      integer   \n",
      "47  public            time        start_time  timestamp without time zone   \n",
      "48  public            time              hour                      integer   \n",
      "49  public            time               day                      integer   \n",
      "50  public            time              week                      integer   \n",
      "51  public            time             month                      integer   \n",
      "52  public            time              year                      integer   \n",
      "53  public            time           weekday                      integer   \n",
      "54  public           users           user_id                      integer   \n",
      "55  public           users        first_name            character varying   \n",
      "56  public           users         last_name            character varying   \n",
      "57  public           users            gender            character varying   \n",
      "58  public           users             level            character varying   \n",
      "\n",
      "    Max Length  Precision  \n",
      "0        256.0        NaN  \n",
      "1        256.0        NaN  \n",
      "2        256.0        NaN  \n",
      "3          NaN       53.0  \n",
      "4          NaN       53.0  \n",
      "5          NaN       32.0  \n",
      "6          NaN        NaN  \n",
      "7          NaN       32.0  \n",
      "8        256.0        NaN  \n",
      "9        256.0        NaN  \n",
      "10       256.0        NaN  \n",
      "11         NaN       32.0  \n",
      "12       256.0        NaN  \n",
      "13       256.0        NaN  \n",
      "14       256.0        NaN  \n",
      "15       256.0        NaN  \n",
      "16       256.0        NaN  \n",
      "17         NaN       32.0  \n",
      "18         NaN       53.0  \n",
      "19       256.0        NaN  \n",
      "20       256.0        NaN  \n",
      "21       256.0        NaN  \n",
      "22       256.0        NaN  \n",
      "23         NaN       32.0  \n",
      "24       256.0        NaN  \n",
      "25         NaN       53.0  \n",
      "26       256.0        NaN  \n",
      "27       256.0        NaN  \n",
      "28       256.0        NaN  \n",
      "29       256.0        NaN  \n",
      "30         NaN       53.0  \n",
      "31         NaN       32.0  \n",
      "32       256.0        NaN  \n",
      "33         NaN       32.0  \n",
      "34         NaN       64.0  \n",
      "35       256.0        NaN  \n",
      "36         NaN       32.0  \n",
      "37         NaN       32.0  \n",
      "38       256.0        NaN  \n",
      "39         NaN       53.0  \n",
      "40         NaN       53.0  \n",
      "41       256.0        NaN  \n",
      "42       256.0        NaN  \n",
      "43       256.0        NaN  \n",
      "44       256.0        NaN  \n",
      "45         NaN       53.0  \n",
      "46         NaN       32.0  \n",
      "47         NaN        NaN  \n",
      "48         NaN       32.0  \n",
      "49         NaN       32.0  \n",
      "50         NaN       32.0  \n",
      "51         NaN       32.0  \n",
      "52         NaN       32.0  \n",
      "53         NaN       32.0  \n",
      "54         NaN       32.0  \n",
      "55       256.0        NaN  \n",
      "56       256.0        NaN  \n",
      "57       256.0        NaN  \n",
      "58       256.0        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Query to show table schemas\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            table_schema,\n",
    "            table_name,\n",
    "            column_name,\n",
    "            data_type,\n",
    "            character_maximum_length,\n",
    "            numeric_precision\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        ORDER BY table_name, ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    schemas = cur.fetchall()\n",
    "    schemas_df = pd.DataFrame(schemas, columns=['Schema', 'Table', 'Column', 'Data Type', 'Max Length', 'Precision'])\n",
    "    print(\"Table schemas in the database:\")\n",
    "    print(schemas_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster3.czgl7wspitsl.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-08df3248a3b917e72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1           NodeType   \n",
       "2      ClusterStatus   \n",
       "3     MasterUsername   \n",
       "4             DBName   \n",
       "5           Endpoint   \n",
       "6              VpcId   \n",
       "7      NumberOfNodes   \n",
       "\n",
       "                                                                                    Value  \n",
       "0                                                                             dwhcluster3  \n",
       "1                                                                               dc2.large  \n",
       "2                                                                               available  \n",
       "3                                                                                 dwhuser  \n",
       "4                                                                                     dwh  \n",
       "5  {'Address': 'dwhcluster3.czgl7wspitsl.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6                                                                   vpc-08df3248a3b917e72  \n",
       "7                                                                                      12  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name='us-west-2',\n",
    "                       aws_access_key_id=config['AWS']['KEY'],\n",
    "                       aws_secret_access_key=config['AWS']['SECRET']\n",
    "                      )\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config['DWH']['DWH_CLUSTER_IDENTIFIER']\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 (Create Table Schemas) is complete:\n",
    "# - All table schemas are defined\n",
    "# - Tables have been created in Redshift\n",
    "# - Schema verification was successful\n",
    "# Moving on to Step 2: ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (ETL Pipeline)\n",
    "\n",
    "Example Output From An ETL Run\n",
    "\n",
    "```\n",
    "roleArn is arn:aws:iam::982052677744:role/dwhRole\n",
    "cluster created\n",
    "1 tries to get Endpoint\n",
    "2 tries to get Endpoint\n",
    "Endpoint is dwhcluster.colkbiahppv4.us-west-2.redshift.amazonaws.com\n",
    "postgresql://dwhAdmin:[REDACTED]@dwhcluster.colkbiahppv4.us-west-2.redshift.amazonaws.com:5439/dev\n",
    "\n",
    "    SELECT COUNT(*) FROM staging_events\n",
    "[8056]\n",
    "\n",
    "    SELECT COUNT(*) FROM staging_songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM songplays\n",
    "[6820]\n",
    "\n",
    "    SELECT COUNT(*) FROM users\n",
    "[104]\n",
    "\n",
    "    SELECT COUNT(*) FROM songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM artists\n",
    "[10025]\n",
    "\n",
    "    SELECT COUNT(*) FROM time\n",
    "[6813]\n",
    "```\n",
    "\n",
    "\n",
    "Note from the instruction\n",
    "- The SERIAL command in Postgres is not supported in Redshift. The equivalent in redshift is IDENTITY(0,1), which you can read more on in the Redshift Create Table Docs(opens in a new tab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log data structure:\n",
      "log-data/\n",
      "log-data/2018/11/2018-11-01-events.json\n",
      "log-data/2018/11/2018-11-02-events.json\n",
      "log-data/2018/11/2018-11-03-events.json\n",
      "log-data/2018/11/2018-11-04-events.json\n",
      "\n",
      "Song data structure:\n",
      "song-data/\n",
      "song-data/A/A/A/TRAAAAK128F9318786.json\n",
      "song-data/A/A/A/TRAAAAV128F421A322.json\n",
      "song-data/A/A/A/TRAAABD128F429CF47.json\n",
      "song-data/A/A/A/TRAAACN128F9355673.json\n"
     ]
    }
   ],
   "source": [
    "# STAGING TABLES\n",
    "\n",
    "## recall from dwh-3.cfg\n",
    "## first question: these are non-partitioned tables or partitioned tables?\n",
    "## these are non-partitioned tables or partitioned tables? Song data and Log data are partitioned tables, as instruction said\n",
    "\"\"\" \n",
    "LOG_DATA='s3://udacity-dend/log-data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song-data'\n",
    "\"\"\"\n",
    "\n",
    "## can we write to check whehther these are non-partitioned tables or partitioned tables?\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3',  # Changed from 'aws-s3' to 's3'\n",
    "                  aws_access_key_id=config['AWS']['KEY'],\n",
    "                  aws_secret_access_key=config['AWS']['SECRET'],\n",
    "                  region_name=config['AWS']['REGION'])\n",
    "\n",
    "# List objects in log-data bucket\n",
    "log_response = s3.list_objects_v2(Bucket='udacity-dend', Prefix='log-data/')\n",
    "song_response = s3.list_objects_v2(Bucket='udacity-dend', Prefix='song-data/')\n",
    "\n",
    "print(\"Log data structure:\")\n",
    "for obj in log_response.get('Contents', [])[:5]:\n",
    "    print(obj['Key'])\n",
    "\n",
    "print(\"\\nSong data structure:\") \n",
    "for obj in song_response.get('Contents', [])[:5]:\n",
    "    print(obj['Key'])\n",
    "\n",
    "# If we see prefixes like year/month/day or artist/year etc,\n",
    "# it indicates partitioned data\n",
    "# If files are flat in the bucket, likely non-partitioned\n",
    "\n",
    "## The log files in the dataset you'll be working with are partitioned by year and month\n",
    "## The song files are partitioned by the first three letters of each song's track ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONPath file content:\n",
      "{\n",
      "  \"jsonpaths\": [\n",
      "    \"$['artist']\",\n",
      "    \"$['auth']\",\n",
      "    \"$['firstName']\",\n",
      "    \"$['gender']\",\n",
      "    \"$['itemInSession']\",\n",
      "    \"$['lastName']\",\n",
      "    \"$['length']\",\n",
      "    \"$['level']\",\n",
      "    \"$['location']\",\n",
      "    \"$['method']\",\n",
      "    \"$['page']\",\n",
      "    \"$['registration']\",\n",
      "    \"$['sessionId']\",\n",
      "    \"$['song']\",\n",
      "    \"$['status']\",\n",
      "    \"$['ts']\",\n",
      "    \"$['userAgent']\",\n",
      "    \"$['userId']\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Log JSON Metadata\n",
    "\"\"\"\n",
    "The log_json_path.json file is used when loading JSON data into Redshift. It specifies the structure of the JSON data so that Redshift can properly parse and load it into the staging tables.\n",
    "\n",
    "In the context of this project, you will need the log_json_path.json file in the COPY command, which is responsible for loading the log data from S3 into the staging tables in Redshift. The log_json_path.json file tells Redshift how to interpret the JSON data and extract the relevant fields. This is essential for further processing and transforming the data into the desired analytics tables.\n",
    "\n",
    "Below is what data is in log_json_path.json.\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "# Get the content of the log_json_path.json file\n",
    "response = s3.get_object(Bucket='udacity-dend', Key='log_json_path.json')\n",
    "jsonpath_content = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "print(\"JSONPath file content:\")\n",
    "print(json.dumps(jsonpath_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song file content:\n",
      "{\n",
      "  \"song_id\": \"SOBLFFE12AF72AA5BA\",\n",
      "  \"num_songs\": 1,\n",
      "  \"title\": \"Scream\",\n",
      "  \"artist_name\": \"Adelitas Way\",\n",
      "  \"artist_latitude\": null,\n",
      "  \"year\": 2009,\n",
      "  \"duration\": 213.9424,\n",
      "  \"artist_id\": \"ARJNIUY12298900C91\",\n",
      "  \"artist_longitude\": null,\n",
      "  \"artist_location\": \"\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Example log file content (first record):\n",
      "{\n",
      "  \"artist\": null,\n",
      "  \"auth\": \"Logged In\",\n",
      "  \"firstName\": \"Theodore\",\n",
      "  \"gender\": \"M\",\n",
      "  \"itemInSession\": 0,\n",
      "  \"lastName\": \"Smith\",\n",
      "  \"length\": null,\n",
      "  \"level\": \"free\",\n",
      "  \"location\": \"Houston-The Woodlands-Sugar Land, TX\",\n",
      "  \"method\": \"GET\",\n",
      "  \"page\": \"Home\",\n",
      "  \"registration\": 1540306145796.0,\n",
      "  \"sessionId\": 154,\n",
      "  \"song\": null,\n",
      "  \"status\": 200,\n",
      "  \"ts\": 1541290555796,\n",
      "  \"userAgent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0\",\n",
      "  \"userId\": \"52\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get sample data from both datasets\n",
    "## Song and Log datasets\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id=config['AWS']['KEY'],\n",
    "                  aws_secret_access_key=config['AWS']['SECRET'],\n",
    "                  region_name=config['AWS']['REGION'])\n",
    "\n",
    "# Get a sample song file\n",
    "try:\n",
    "    song_response = s3.get_object(Bucket='udacity-dend', \n",
    "                                 Key='song-data/A/A/A/TRAAAAK128F9318786.json')\n",
    "    song_content = json.loads(song_response['Body'].read().decode('utf-8'))\n",
    "    print(\"Example song file content:\")\n",
    "    print(json.dumps(song_content, indent=2))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting song file: {e}\")\n",
    "\n",
    "#  Get a sample log file\n",
    "try:\n",
    "    log_response = s3.get_object(Bucket='udacity-dend', \n",
    "                                Key='log-data/2018/11/2018-11-04-events.json')\n",
    "    # Read the content of the log file\n",
    "    log_content = log_response['Body'].read().decode('utf-8')\n",
    "    # split the content by new lines\n",
    "    log_records = [json.loads(line) for line in log_content.strip().split('\\n')]\n",
    "    \n",
    "    print(\"Example log file content (first record):\")\n",
    "    print(json.dumps(log_records[0], indent=2))  # show first record\n",
    "except Exception as e:\n",
    "    print(f\"Error getting log file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Write ETL queries for staging tables\n",
    "\n",
    "## Step 2.2: for Final tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2.1 Review the queries for staging tables\n",
    "\n",
    "## note: Added .strip(\"'\") to remove the single quotes from the ARN value in your config file\n",
    "\n",
    "## log data (partitioned by year/month)\n",
    "staging_events_copy = \"\"\"\n",
    "    COPY staging_events FROM 's3://udacity-dend/log-data'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    json {}\n",
    "    region 'us-west-2'\n",
    "    maxerror 10000\n",
    "    compupdate off\n",
    "    statupdate off;\n",
    "\"\"\".format(config['IAM_ROLE']['ARN'].strip(\"'\"), config['S3']['LOG_JSONPATH'])\n",
    "\n",
    "\n",
    "## song data (partitioned by first 3 letters of track ID)\n",
    "\n",
    "\n",
    "staging_songs_copy = \"\"\"\n",
    "    COPY staging_songs FROM 's3://udacity-dend/song-data'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    json 'auto'\n",
    "    region 'us-west-2'\n",
    "    maxerror 10000\n",
    "    compupdate off\n",
    "    statupdate off;\n",
    "\"\"\".format(config['IAM_ROLE']['ARN'].strip(\"'\"))\n",
    "\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TABLES\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT \n",
    "DISTINCT TIMESTAMP 'epoch' + se.ts/1000 * INTERVAL '1 second' AS start_time,\n",
    "       se.userId AS user_id,\n",
    "       se.level,\n",
    "       ss.song_id,\n",
    "       ss.artist_id,\n",
    "       se.sessionId AS session_id,\n",
    "       se.location,\n",
    "       se.userAgent AS user_agent\n",
    "FROM staging_events se\n",
    "JOIN staging_songs ss\n",
    "ON se.song = ss.title AND se.artist = ss.artist_name\n",
    "WHERE se.page = 'NextSong'\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT userId AS user_id,\n",
    "       firstName AS first_name,\n",
    "       lastName AS last_name,\n",
    "       gender,\n",
    "       level\n",
    "FROM staging_events\n",
    "WHERE userId IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT song_id,\n",
    "       title,\n",
    "       artist_id,\n",
    "       year,\n",
    "       duration\n",
    "FROM staging_songs\n",
    "WHERE song_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT artist_id,\n",
    "       artist_name AS name,\n",
    "       artist_location AS location,\n",
    "       artist_latitude AS latitude,\n",
    "       artist_longitude AS longitude\n",
    "FROM staging_songs\n",
    "WHERE artist_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "SELECT start_time,\n",
    "       EXTRACT(hour FROM start_time),\n",
    "       EXTRACT(day FROM start_time),\n",
    "       EXTRACT(week FROM start_time),\n",
    "       EXTRACT(month FROM start_time),\n",
    "       EXTRACT(year FROM start_time),\n",
    "       EXTRACT(weekday FROM start_time)\n",
    "FROM (SELECT DISTINCT TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second' AS start_time\n",
    "      FROM staging_events)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully\n",
      "Executing query: \n",
      "    COPY staging_events FROM 's3://udacity-dend/log-data'\n",
      "    credentials 'aws_iam_role=arn:aws:iam...\n",
      "Executing query: \n",
      "    COPY staging_songs FROM 's3://udacity-dend/song-data'\n",
      "    credentials 'aws_iam_role=arn:aws:iam...\n",
      "Staging tables loaded successfully:\n",
      "staging_events: 8056 rows\n",
      "staging_songs: 385212 rows\n",
      "Executing query: \n",
      "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_a...\n",
      "Executing query: \n",
      "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
      "SELECT DISTINCT userId AS user_id...\n",
      "Executing query: \n",
      "INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
      "SELECT DISTINCT song_id,\n",
      "       title...\n",
      "Executing query: \n",
      "INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
      "SELECT DISTINCT artist_id,\n",
      "   ...\n",
      "Executing query: \n",
      "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
      "SELECT start_time,\n",
      "       EXTR...\n",
      "Analytics tables loaded successfully:\n",
      "songplays: 6961 rows\n",
      "users: 105 rows\n",
      "songs: 384955 rows\n",
      "artists: 45262 rows\n",
      "time: 8023 rows\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    try:\n",
    "        for query in copy_table_queries:\n",
    "            print(f\"Executing query: {query[:100]}...\")  # Print first 100 chars for debugging\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "        # Validate staging data\n",
    "        cur.execute(\"SELECT COUNT(*) FROM staging_events\")\n",
    "        events_count = cur.fetchone()[0]\n",
    "        cur.execute(\"SELECT COUNT(*) FROM staging_songs\")\n",
    "        songs_count = cur.fetchone()[0]\n",
    "        print(f\"Staging tables loaded successfully:\")\n",
    "        print(f\"staging_events: {events_count} rows\")\n",
    "        print(f\"staging_songs: {songs_count} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading staging tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    try:\n",
    "        for query in insert_table_queries:\n",
    "            print(f\"Executing query: {query[:100]}...\")  # Print first 100 chars for debugging\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "        # Validate analytics tables\n",
    "        validation_queries = [\n",
    "            (\"songplays\", \"SELECT COUNT(*) FROM songplays\"),\n",
    "            (\"users\", \"SELECT COUNT(*) FROM users\"),\n",
    "            (\"songs\", \"SELECT COUNT(*) FROM songs\"),\n",
    "            (\"artists\", \"SELECT COUNT(*) FROM artists\"),\n",
    "            (\"time\", \"SELECT COUNT(*) FROM time\")\n",
    "        ]\n",
    "        \n",
    "        print(\"Analytics tables loaded successfully:\")\n",
    "        for table, query in validation_queries:\n",
    "            cur.execute(query)\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"{table}: {count} rows\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting into tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Main ETL execution\n",
    "conn = None\n",
    "cur = None\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    print(\"Connected to Redshift successfully\")\n",
    "    \n",
    "    # Execute ETL process\n",
    "    load_staging_tables(cur, conn)\n",
    "    insert_tables(cur, conn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during ETL process: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT COUNT(*) FROM staging_events\n",
      "[8056]\n",
      "\n",
      "    SELECT COUNT(*) FROM staging_songs\n",
      "[385212]\n",
      "\n",
      "    SELECT COUNT(*) FROM songplays\n",
      "[6961]\n",
      "\n",
      "    SELECT COUNT(*) FROM users\n",
      "[105]\n",
      "\n",
      "    SELECT COUNT(*) FROM songs\n",
      "[384955]\n",
      "\n",
      "    SELECT COUNT(*) FROM artists\n",
      "[45262]\n",
      "\n",
      "    SELECT COUNT(*) FROM time\n",
      "[8023]\n"
     ]
    }
   ],
   "source": [
    "# Check row counts for all tables\n",
    "table_queries = [\n",
    "    \"SELECT COUNT(*) FROM staging_events\",\n",
    "    \"SELECT COUNT(*) FROM staging_songs\",\n",
    "    \"SELECT COUNT(*) FROM songplays\",\n",
    "    \"SELECT COUNT(*) FROM users\",\n",
    "    \"SELECT COUNT(*) FROM songs\",\n",
    "    \"SELECT COUNT(*) FROM artists\",\n",
    "    \"SELECT COUNT(*) FROM time\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Execute each count query and print results\n",
    "    for query in table_queries:\n",
    "        print(f\"\\n    {query}\")\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "        print(f\"[{result[0]}]\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking row counts: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Row Counts:\n",
      "--------------------------------------------------\n",
      "Staging Events       |      8,056 rows\n",
      "Staging Songs        |    385,212 rows\n",
      "Songplays (Fact)     |      6,961 rows\n",
      "Users (Dim)          |        105 rows\n",
      "Songs (Dim)          |    384,955 rows\n",
      "Artists (Dim)        |     45,262 rows\n",
      "Time (Dim)           |      8,023 rows\n",
      "--------------------------------------------------\n",
      "\n",
      "Data Quality Checks:\n",
      "--------------------------------------------------\n",
      "Null songplay_id in songplays  |          0 rows\n",
      "Null user_id in users      |          0 rows\n",
      "Null song_id in songs      |          0 rows\n",
      "Null artist_id in artists    |          0 rows\n",
      "Null start_time in time       |          0 rows\n"
     ]
    }
   ],
   "source": [
    "# # simplified the code by removing the non-null checks and focusing only on primary key validation since that's the most critical aspect\n",
    "\n",
    "def check_table_counts():\n",
    "    # Dictionary of tables and their corresponding queries\n",
    "    table_counts = {\n",
    "        'Staging Events': \"SELECT COUNT(*) FROM staging_events\",\n",
    "        'Staging Songs': \"SELECT COUNT(*) FROM staging_songs\",\n",
    "        'Songplays (Fact)': \"SELECT COUNT(*) FROM songplays\",\n",
    "        'Users (Dim)': \"SELECT COUNT(*) FROM users\",\n",
    "        'Songs (Dim)': \"SELECT COUNT(*) FROM songs\",\n",
    "        'Artists (Dim)': \"SELECT COUNT(*) FROM artists\",\n",
    "        'Time (Dim)': \"SELECT COUNT(*) FROM time\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Connect to Redshift cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=config['CLUSTER']['HOST'],\n",
    "            dbname=config['CLUSTER']['DB_NAME'],\n",
    "            user=config['CLUSTER']['DB_USER'],\n",
    "            password=config['CLUSTER']['DB_PASSWORD'],\n",
    "            port=int(config['CLUSTER']['DB_PORT'])\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nTable Row Counts:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Execute each count query and print results\n",
    "        for table_name, query in table_counts.items():\n",
    "            cur.execute(query)\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"{table_name:<20} | {count:>10,} rows\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Additional validation queries\n",
    "        print(\"\\nData Quality Checks:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Check for null values in primary keys\n",
    "        null_checks = {\n",
    "            'songplays': 'songplay_id',\n",
    "            'users': 'user_id',\n",
    "            'songs': 'song_id',\n",
    "            'artists': 'artist_id',\n",
    "            'time': 'start_time'\n",
    "        }\n",
    "        \n",
    "        for table, pk in null_checks.items():\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {table} WHERE {pk} IS NULL\")\n",
    "            null_count = cur.fetchone()[0]\n",
    "            print(f\"Null {pk} in {table:<10} | {null_count:>10} rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during validation: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "# Run the validation\n",
    "check_table_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Provide example queries and results for song play analysis. We do not provide you any of these. You, as part of the Data Engineering team were tasked to build this ETL. Thorough study has gone into the star schema, tables, and columns required. The ETL will be effective and provide the data and in the format required. However, as an exercise, it seems almost silly to NOT show SOME examples of potential queries that could be ran by the users. PLEASE use your imagination here. For example, what is the most played song? When is the highest usage time of day by hour for songs? It would not take much to imagine what types of questions that corporate users of the system would find interesting. Including those queries and the answers makes your project far more compelling when using it as an example of your work to people / companies that would be interested. You could simply have a section of sql_queries.py that is executed after the load is done that prints a question and then the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkify Music Streaming Analytics\n",
      "==================================================\n",
      "\n",
      "Most Popular Songs\n",
      "------------------\n",
      "         title                          artist  play_count\n",
      "   Greece 2000                    Three Drives         110\n",
      "      Stronger                      Kanye West          56\n",
      "   Greece 2000             3 Drives On A Vinyl          55\n",
      "You're The One                   Dwight Yoakam          37\n",
      "You're The One Dwight Yoakam with Kelly Willis          37\n",
      "\n",
      "\n",
      "\n",
      "Peak Usage Hours\n",
      "----------------\n",
      " hour  plays\n",
      "   16    575\n",
      "   17    515\n",
      "   18    502\n",
      "   15    473\n",
      "   14    445\n",
      "   19    376\n",
      "   20    367\n",
      "   11    338\n",
      "   13    322\n",
      "   10    319\n",
      "   12    304\n",
      "   21    284\n",
      "    9    276\n",
      "   22    220\n",
      "    8    217\n",
      "   23    205\n",
      "    6    195\n",
      "    7    188\n",
      "    5    165\n",
      "    1    157\n",
      "    0    156\n",
      "    4    141\n",
      "    2    112\n",
      "    3    109\n",
      "\n",
      "\n",
      "\n",
      "User Activity by Level\n",
      "----------------------\n",
      "level  user_count  total_plays  avg_plays_per_user\n",
      " free          82         1263                15.4\n",
      " paid          22         5698               259.0\n",
      "\n",
      "\n",
      "\n",
      "Most Active Users\n",
      "-----------------\n",
      "first_name last_name level  play_count\n",
      "     Chloe    Cuevas  paid         690\n",
      "     Chloe    Cuevas  free         690\n",
      "     Tegan    Levine  paid         687\n",
      "     Tegan    Levine  free         687\n",
      "      Kate   Harrell  paid         552\n",
      "      Lily      Koch  free         484\n",
      "      Lily      Koch  paid         484\n",
      "    Aleena     Kirby  paid         417\n",
      "Jacqueline     Lynch  free         347\n",
      "Jacqueline     Lynch  paid         347\n",
      "\n",
      "\n",
      "\n",
      "Popular Music by Day of Week\n",
      "----------------------------\n",
      " weekday  total_plays  unique_users\n",
      "       0          391            39\n",
      "       1         1013            58\n",
      "       2         1094            57\n",
      "       3         1410            60\n",
      "       4         1078            56\n",
      "       5         1320            62\n",
      "       6          655            44\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_analytical_queries():\n",
    "    # Dictionary of analytical queries with their descriptions\n",
    "    analytical_queries = {\n",
    "        \"Most Popular Songs\": \"\"\"\n",
    "            SELECT s.title, a.name as artist, COUNT(*) as play_count\n",
    "            FROM songplays sp\n",
    "            JOIN songs s ON sp.song_id = s.song_id\n",
    "            JOIN artists a ON sp.artist_id = a.artist_id\n",
    "            GROUP BY s.title, a.name\n",
    "            ORDER BY play_count DESC\n",
    "            LIMIT 5;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Peak Usage Hours\": \"\"\"\n",
    "            SELECT t.hour, COUNT(*) as plays\n",
    "            FROM songplays sp\n",
    "            JOIN time t ON sp.start_time = t.start_time\n",
    "            GROUP BY t.hour\n",
    "            ORDER BY plays DESC\n",
    "            LIMIT 24;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"User Activity by Level\": \"\"\"\n",
    "            SELECT level, COUNT(DISTINCT user_id) as user_count,\n",
    "                   COUNT(*) as total_plays,\n",
    "                   ROUND(COUNT(*) / COUNT(DISTINCT user_id)::float, 2) as avg_plays_per_user\n",
    "            FROM songplays\n",
    "            GROUP BY level;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Most Active Users\": \"\"\"\n",
    "            SELECT u.first_name, u.last_name, u.level,\n",
    "                   COUNT(*) as play_count\n",
    "            FROM songplays sp\n",
    "            JOIN users u ON sp.user_id = u.user_id\n",
    "            GROUP BY u.user_id, u.first_name, u.last_name, u.level\n",
    "            ORDER BY play_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Popular Music by Day of Week\": \"\"\"\n",
    "            SELECT t.weekday,\n",
    "                   COUNT(*) as total_plays,\n",
    "                   COUNT(DISTINCT sp.user_id) as unique_users\n",
    "            FROM songplays sp\n",
    "            JOIN time t ON sp.start_time = t.start_time\n",
    "            GROUP BY t.weekday\n",
    "            ORDER BY t.weekday;\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Connect to Redshift\n",
    "        conn = psycopg2.connect(\n",
    "            host=config['CLUSTER']['HOST'],\n",
    "            dbname=config['CLUSTER']['DB_NAME'],\n",
    "            user=config['CLUSTER']['DB_USER'],\n",
    "            password=config['CLUSTER']['DB_PASSWORD'],\n",
    "            port=int(config['CLUSTER']['DB_PORT'])\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        print(\"\\nSparkify Music Streaming Analytics\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Execute each analytical query and display results\n",
    "        for title, query in analytical_queries.items():\n",
    "            print(f\"\\n{title}\")\n",
    "            print(\"-\" * len(title))\n",
    "            \n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            # Get column names from cursor description\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            \n",
    "            # Create DataFrame for prettier display\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            print(df.to_string(index=False))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running analytical queries: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "# Run the analytical queries\n",
    "run_analytical_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step: Project Rubric requirements\n",
    "\n",
    "## Readme.md\n",
    "\n",
    "The project shows proper use of documentation.\n",
    "\n",
    "The README file includes a summary of the project, how to run the Python scripts, and an explanation of the files in the repository. Comments are used effectively and each function has a docstring.\n",
    "\n",
    "\n",
    "## Clean modular Code\n",
    "\n",
    "Scripts have an intuitive, easy-to-follow structure with code separated into logical functions. Naming for variables and functions follows the PEP8 style guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Good practice\n",
    "\n",
    "## Create and Delete Cluster\n",
    "\n",
    "## IAM roles\n",
    "\n",
    "## Access Key: Create and Delete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
