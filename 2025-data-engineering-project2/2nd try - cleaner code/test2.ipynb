{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify S3 to Redshift ETL Project\n",
    "\n",
    "## Project Overview\n",
    "Sparkify, a music streaming startup, is migrating its data processes to the cloud. The goal is to build an ETL pipeline that extracts data from AWS S3, stages it in Amazon Redshift, and transforms it into a star schema for analytical queries.\n",
    "\n",
    "## Data Sources\n",
    "- **S3 Buckets:**\n",
    "  - `song_data`: JSON metadata about songs.\n",
    "  - `log_data`: JSON logs of user activity.\n",
    "  - `log_json_path`: JSON format configuration for parsing.\n",
    "\n",
    "## ETL Process\n",
    "1. **Extract**: Load raw data from S3 into Redshift staging tables.\n",
    "2. **Transform**: Clean and structure data into facts and dimensions.\n",
    "3. **Load**: Insert processed data into an analytical star schema.\n",
    "\n",
    "## Data Warehouse Schema (Star Schema)\n",
    "- **Fact Table:** `songplays` (stores song play events)\n",
    "- **Dimension Tables:**\n",
    "  - `users` (user information)\n",
    "  - `songs` (song details)\n",
    "  - `artists` (artist details)\n",
    "  - `time` (timestamp breakdown)\n",
    "\n",
    "## Redshift Staging Tables\n",
    "- `staging_songs` (raw song data)\n",
    "- `staging_logs` (raw log data)\n",
    "\n",
    "## Technologies Used\n",
    "- **AWS S3**: Data storage\n",
    "- **AWS Redshift**: Cloud-based data warehouse\n",
    "- **Python**: ETL scripting\n",
    "- **SQL**: Data transformation and querying\n",
    "\n",
    "## Key Considerations\n",
    "- Bulk data loading using `COPY` for efficiency.\n",
    "- Role-based access control for security.\n",
    "- Optimized schema design for analytical performance.\n",
    "\n",
    "## Goal\n",
    "Enable Sparkify's analytics team to gain insights on user behavior and song preferences efficiently using a scalable cloud-based data warehouse.\n",
    "\n",
    "\n",
    "# Project Summary\n",
    "\n",
    "## Project Datasets\n",
    "\n",
    "This project utilizes two datasets stored in Amazon S3:\n",
    "- **Song Data**: `s3://udacity-dend/song_data`\n",
    "- **Log Data**: `s3://udacity-dend/log_data`\n",
    "- **Metadata** (for parsing log data): `s3://udacity-dend/log_json_path.json`\n",
    "\n",
    "The datasets are structured in JSON format:\n",
    "- **Song Dataset**: Extracted from the Million Song Dataset, containing song metadata.\n",
    "- **Log Dataset**: Simulated event logs from a music streaming app.\n",
    "\n",
    "## Schema for Song Play Analysis\n",
    "\n",
    "A star schema is used for optimizing queries:\n",
    "\n",
    "### **Fact Table**\n",
    "- **songplays** - records of song plays\n",
    "  - `songplay_id`, `start_time`, `user_id`, `level`, `song_id`, `artist_id`, `session_id`, `location`, `user_agent`\n",
    "\n",
    "### **Dimension Tables**\n",
    "- **users** - user details\n",
    "  - `user_id`, `first_name`, `last_name`, `gender`, `level`\n",
    "- **songs** - song details\n",
    "  - `song_id`, `title`, `artist_id`, `year`, `duration`\n",
    "- **artists** - artist details\n",
    "  - `artist_id`, `name`, `location`, `latitude`, `longitude`\n",
    "- **time** - timestamp details\n",
    "  - `start_time`, `hour`, `day`, `week`, `month`, `year`, `weekday`\n",
    "\n",
    "## Project Steps\n",
    "\n",
    "### **1. Create Table Schemas**\n",
    "- Define table schemas in `sql_queries.py`\n",
    "- Implement table creation logic in `create_table.py`\n",
    "- Configure Redshift cluster and IAM roles\n",
    "\n",
    "### **2. Build ETL Pipeline**\n",
    "- Extract data from S3 and load into Redshift staging tables (`etl.py`)\n",
    "- Transform and load data into analytics tables\n",
    "- Validate data with test queries\n",
    "\n",
    "### **3. Document Process**\n",
    "- Explain database design and ETL process in `README.md`\n",
    "- Provide example queries for analytical insights\n",
    "\n",
    "## Project Files\n",
    "- `create_table.py` - Defines and creates database tables\n",
    "- `etl.py` - Extracts, transforms, and loads data into Redshift\n",
    "- `sql_queries.py` - SQL statements for table creation and data loading\n",
    "- `README.md` - Documentation of project details\n",
    "\n",
    "## Notes\n",
    "- Redshift does not support `SERIAL`; use `IDENTITY(0,1)` instead.\n",
    "- Do not include AWS credentials in the code.\n",
    "\n",
    "## Next Steps\n",
    "- Run ETL pipeline and validate results\n",
    "- Optimize query performance\n",
    "- Expand dataset integration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Creating Redshift Cluster\n",
    "\n",
    "Tihs part we can look at code from homework \"L3 exercise 2 - IaC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Create Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dwh-3.cfg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "# CONFIG\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh-3.cfg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABle IF EXISTS staging_events;\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs;\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE  IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_events\n",
    "(\n",
    "    artist VARCHAR,\n",
    "    auth VARCHAR,\n",
    "    firstName VARCHAR(50),\n",
    "    gender CHAR,\n",
    "    itemInSession INTEGER,\n",
    "    lastName VARCHAR(50),\n",
    "    length FLOAT,\n",
    "    level VARCHAR,\n",
    "    location VARCHAR,\n",
    "    method VARCHAR,\n",
    "    page VARCHAR,\n",
    "    registration FLOAT,\n",
    "    sessionId INTEGER,\n",
    "    song VARCHAR,\n",
    "    status INTEGER,\n",
    "    ts BIGINT,\n",
    "    userAgent VARCHAR,\n",
    "    userId INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_songs\n",
    "(\n",
    "    num_songs INTEGER,\n",
    "    artist_id VARCHAR,\n",
    "    artist_latitude FLOAT,\n",
    "    artist_longitude FLOAT,\n",
    "    artist_location VARCHAR,\n",
    "    artist_name VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    title VARCHAR,\n",
    "    duration FLOAT,\n",
    "    year FLOAT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "songplay_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplays\n",
    "(\n",
    "    songplay_id INTEGER IDENTITY (1, 1) PRIMARY KEY ,\n",
    "    start_time TIMESTAMP,\n",
    "    user_id INTEGER,\n",
    "    level VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    artist_id VARCHAR,\n",
    "    session_id INTEGER,\n",
    "    location VARCHAR,\n",
    "    user_agent VARCHAR\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY ( start_time )\n",
    "SORTKEY ( start_time );\n",
    "\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users\n",
    "(\n",
    "    userId INTEGER PRIMARY KEY,\n",
    "    firsname VARCHAR(50),\n",
    "    lastname VARCHAR(50),\n",
    "    gender CHAR(1) ENCODE BYTEDICT,\n",
    "    level VARCHAR ENCODE BYTEDICT\n",
    ")\n",
    "SORTKEY (userId);\n",
    "\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songs\n",
    "(\n",
    "    song_id VARCHAR PRIMARY KEY,\n",
    "    title VARCHAR,\n",
    "    artist_id VARCHAR,\n",
    "    year INTEGER ENCODE BYTEDICT,\n",
    "    duration FLOAT\n",
    ")\n",
    "SORTKEY (song_id);\n",
    "\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artists\n",
    "(\n",
    "    artist_id VARCHAR PRIMARY KEY ,\n",
    "    name VARCHAR,\n",
    "    location VARCHAR,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT\n",
    ")\n",
    "SORTKEY (artist_id);\n",
    "\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS time\n",
    "(\n",
    "    start_time  TIMESTAMP PRIMARY KEY ,\n",
    "    hour INTEGER,\n",
    "    day INTEGER,\n",
    "    week INTEGER,\n",
    "    month INTEGER,\n",
    "    year INTEGER ENCODE BYTEDICT ,\n",
    "    weekday VARCHAR(9) ENCODE BYTEDICT\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY ( start_time )\n",
    "SORTKEY (start_time);\n",
    "\"\"\")\n",
    "\n",
    "# STAGING TABLES\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "COPY staging_events\n",
    "FROM {}\n",
    "iam_role {}\n",
    "FORMAT AS json {};\n",
    "\"\"\").format(config['S3']['LOG_DATA'], config['IAM_ROLE']['ARN'], config['S3']['LOG_JSONPATH'])\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "COPY staging_songs\n",
    "FROM {}\n",
    "iam_role {}\n",
    "FORMAT AS json 'auto';\n",
    "\"\"\").format(config['S3']['SONG_DATA'], config['IAM_ROLE']['ARN'])\n",
    "\n",
    "# FINAL TABLES\n",
    "\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO songplays (START_TIME, USER_ID, LEVEL, SONG_ID, ARTIST_ID, SESSION_ID, LOCATION, USER_AGENT)\n",
    "SELECT DISTINCT\n",
    "       TIMESTAMP 'epoch' + (se.ts / 1000) * INTERVAL '1 second' as start_time,\n",
    "                se.userId,\n",
    "                se.level,\n",
    "                ss.song_id,\n",
    "                ss.artist_id,\n",
    "                se.sessionId,\n",
    "                se.location,\n",
    "                se.userAgent\n",
    "FROM staging_songs ss\n",
    "INNER JOIN staging_events se\n",
    "ON (ss.title = se.song AND se.artist = ss.artist_name)\n",
    "AND se.page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO users\n",
    "SELECT DISTINCT userId, firstName, lastName, gender, level\n",
    "FROM staging_events\n",
    "WHERE userId IS NOT NULL\n",
    "AND page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO songs\n",
    "SELECT\n",
    "    DISTINCT song_id, title, artist_id, year, duration\n",
    "FROM staging_songs\n",
    "WHERE song_id IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO artists\n",
    "SELECT\n",
    "    DISTINCT artist_id, artist_name, artist_location, artist_latitude, artist_longitude\n",
    "FROM staging_songs;\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "insert into time\n",
    "SELECT DISTINCT\n",
    "       TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second' as start_time,\n",
    "       EXTRACT(HOUR FROM start_time) AS hour,\n",
    "       EXTRACT(DAY FROM start_time) AS day,\n",
    "       EXTRACT(WEEKS FROM start_time) AS week,\n",
    "       EXTRACT(MONTH FROM start_time) AS month,\n",
    "       EXTRACT(YEAR FROM start_time) AS year,\n",
    "       to_char(start_time, 'Day') AS weekday\n",
    "FROM staging_events;\n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully\n",
      "Tables dropped successfully\n",
      "Tables created successfully\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    try:\n",
    "        for query in drop_table_queries:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print(\"Tables dropped successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Error dropping tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    try:\n",
    "        for query in create_table_queries:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print(\"Tables created successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "conn = None\n",
    "cur = None\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])  # Convert port to integer\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    print(\"Connected to Redshift successfully\")\n",
    "    \n",
    "    # Drop and create tables\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error connecting to Redshift: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "Show the tables in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table schemas in the database:\n",
      "    Schema           Table            Column                    Data Type  \\\n",
      "0   public         artists         artist_id            character varying   \n",
      "1   public         artists              name            character varying   \n",
      "2   public         artists          location            character varying   \n",
      "3   public         artists          latitude             double precision   \n",
      "4   public         artists         longitude             double precision   \n",
      "5   public       songplays       songplay_id                      integer   \n",
      "6   public       songplays        start_time  timestamp without time zone   \n",
      "7   public       songplays           user_id                      integer   \n",
      "8   public       songplays             level            character varying   \n",
      "9   public       songplays           song_id            character varying   \n",
      "10  public       songplays         artist_id            character varying   \n",
      "11  public       songplays        session_id                      integer   \n",
      "12  public       songplays          location            character varying   \n",
      "13  public       songplays        user_agent            character varying   \n",
      "14  public           songs           song_id            character varying   \n",
      "15  public           songs             title            character varying   \n",
      "16  public           songs         artist_id            character varying   \n",
      "17  public           songs              year                      integer   \n",
      "18  public           songs          duration             double precision   \n",
      "19  public  staging_events            artist            character varying   \n",
      "20  public  staging_events              auth            character varying   \n",
      "21  public  staging_events         firstname            character varying   \n",
      "22  public  staging_events            gender                    character   \n",
      "23  public  staging_events     iteminsession                      integer   \n",
      "24  public  staging_events          lastname            character varying   \n",
      "25  public  staging_events            length             double precision   \n",
      "26  public  staging_events             level            character varying   \n",
      "27  public  staging_events          location            character varying   \n",
      "28  public  staging_events            method            character varying   \n",
      "29  public  staging_events              page            character varying   \n",
      "30  public  staging_events      registration             double precision   \n",
      "31  public  staging_events         sessionid                      integer   \n",
      "32  public  staging_events              song            character varying   \n",
      "33  public  staging_events            status                      integer   \n",
      "34  public  staging_events                ts                       bigint   \n",
      "35  public  staging_events         useragent            character varying   \n",
      "36  public  staging_events            userid                      integer   \n",
      "37  public   staging_songs         num_songs                      integer   \n",
      "38  public   staging_songs         artist_id            character varying   \n",
      "39  public   staging_songs   artist_latitude             double precision   \n",
      "40  public   staging_songs  artist_longitude             double precision   \n",
      "41  public   staging_songs   artist_location            character varying   \n",
      "42  public   staging_songs       artist_name            character varying   \n",
      "43  public   staging_songs           song_id            character varying   \n",
      "44  public   staging_songs             title            character varying   \n",
      "45  public   staging_songs          duration             double precision   \n",
      "46  public   staging_songs              year             double precision   \n",
      "47  public            time        start_time  timestamp without time zone   \n",
      "48  public            time              hour                      integer   \n",
      "49  public            time               day                      integer   \n",
      "50  public            time              week                      integer   \n",
      "51  public            time             month                      integer   \n",
      "52  public            time              year                      integer   \n",
      "53  public            time           weekday            character varying   \n",
      "54  public           users            userid                      integer   \n",
      "55  public           users          firsname            character varying   \n",
      "56  public           users          lastname            character varying   \n",
      "57  public           users            gender                    character   \n",
      "58  public           users             level            character varying   \n",
      "\n",
      "    Max Length  Precision  \n",
      "0        256.0        NaN  \n",
      "1        256.0        NaN  \n",
      "2        256.0        NaN  \n",
      "3          NaN       53.0  \n",
      "4          NaN       53.0  \n",
      "5          NaN       32.0  \n",
      "6          NaN        NaN  \n",
      "7          NaN       32.0  \n",
      "8        256.0        NaN  \n",
      "9        256.0        NaN  \n",
      "10       256.0        NaN  \n",
      "11         NaN       32.0  \n",
      "12       256.0        NaN  \n",
      "13       256.0        NaN  \n",
      "14       256.0        NaN  \n",
      "15       256.0        NaN  \n",
      "16       256.0        NaN  \n",
      "17         NaN       32.0  \n",
      "18         NaN       53.0  \n",
      "19       256.0        NaN  \n",
      "20       256.0        NaN  \n",
      "21        50.0        NaN  \n",
      "22         1.0        NaN  \n",
      "23         NaN       32.0  \n",
      "24        50.0        NaN  \n",
      "25         NaN       53.0  \n",
      "26       256.0        NaN  \n",
      "27       256.0        NaN  \n",
      "28       256.0        NaN  \n",
      "29       256.0        NaN  \n",
      "30         NaN       53.0  \n",
      "31         NaN       32.0  \n",
      "32       256.0        NaN  \n",
      "33         NaN       32.0  \n",
      "34         NaN       64.0  \n",
      "35       256.0        NaN  \n",
      "36         NaN       32.0  \n",
      "37         NaN       32.0  \n",
      "38       256.0        NaN  \n",
      "39         NaN       53.0  \n",
      "40         NaN       53.0  \n",
      "41       256.0        NaN  \n",
      "42       256.0        NaN  \n",
      "43       256.0        NaN  \n",
      "44       256.0        NaN  \n",
      "45         NaN       53.0  \n",
      "46         NaN       53.0  \n",
      "47         NaN        NaN  \n",
      "48         NaN       32.0  \n",
      "49         NaN       32.0  \n",
      "50         NaN       32.0  \n",
      "51         NaN       32.0  \n",
      "52         NaN       32.0  \n",
      "53         9.0        NaN  \n",
      "54         NaN       32.0  \n",
      "55        50.0        NaN  \n",
      "56        50.0        NaN  \n",
      "57         1.0        NaN  \n",
      "58       256.0        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Query to show table schemas\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            table_schema,\n",
    "            table_name,\n",
    "            column_name,\n",
    "            data_type,\n",
    "            character_maximum_length,\n",
    "            numeric_precision\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "        ORDER BY table_name, ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    schemas = cur.fetchall()\n",
    "    schemas_df = pd.DataFrame(schemas, columns=['Schema', 'Table', 'Column', 'Data Type', 'Max Length', 'Precision'])\n",
    "    print(\"Table schemas in the database:\")\n",
    "    print(schemas_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster2.czgl7wspitsl.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-08df3248a3b917e72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1           NodeType   \n",
       "2      ClusterStatus   \n",
       "3     MasterUsername   \n",
       "4             DBName   \n",
       "5           Endpoint   \n",
       "6              VpcId   \n",
       "7      NumberOfNodes   \n",
       "\n",
       "                                                                                    Value  \n",
       "0                                                                             dwhcluster2  \n",
       "1                                                                               dc2.large  \n",
       "2                                                                               available  \n",
       "3                                                                                 dwhuser  \n",
       "4                                                                                     dwh  \n",
       "5  {'Address': 'dwhcluster2.czgl7wspitsl.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6                                                                   vpc-08df3248a3b917e72  \n",
       "7                                                                                       8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a Redshift client\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name='us-west-2',\n",
    "                       aws_access_key_id=config['AWS']['KEY'],\n",
    "                       aws_secret_access_key=config['AWS']['SECRET']\n",
    "                      )\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config['DWH']['DWH_CLUSTER_IDENTIFIER']\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 (Create Table Schemas) is complete:\n",
    "# - All table schemas are defined\n",
    "# - Tables have been created in Redshift\n",
    "# - Schema verification was successful\n",
    "# Moving on to Step 2: ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (ETL Pipeline)\n",
    "\n",
    "Example Output From An ETL Run\n",
    "\n",
    "```\n",
    "roleArn is arn:aws:iam::982052677744:role/dwhRole\n",
    "cluster created\n",
    "1 tries to get Endpoint\n",
    "2 tries to get Endpoint\n",
    "Endpoint is dwhcluster.colkbiahppv4.us-west-2.redshift.amazonaws.com\n",
    "postgresql://dwhAdmin:[REDACTED]@dwhcluster.colkbiahppv4.us-west-2.redshift.amazonaws.com:5439/dev\n",
    "\n",
    "    SELECT COUNT(*) FROM staging_events\n",
    "[8056]\n",
    "\n",
    "    SELECT COUNT(*) FROM staging_songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM songplays\n",
    "[6820]\n",
    "\n",
    "    SELECT COUNT(*) FROM users\n",
    "[104]\n",
    "\n",
    "    SELECT COUNT(*) FROM songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM artists\n",
    "[10025]\n",
    "\n",
    "    SELECT COUNT(*) FROM time\n",
    "[6813]\n",
    "```\n",
    "\n",
    "\n",
    "Note from the instruction\n",
    "- The SERIAL command in Postgres is not supported in Redshift. The equivalent in redshift is IDENTITY(0,1), which you can read more on in the Redshift Create Table Docs(opens in a new tab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log data structure:\n",
      "log-data/\n",
      "log-data/2018/11/2018-11-01-events.json\n",
      "log-data/2018/11/2018-11-02-events.json\n",
      "log-data/2018/11/2018-11-03-events.json\n",
      "log-data/2018/11/2018-11-04-events.json\n",
      "\n",
      "Song data structure:\n",
      "song-data/\n",
      "song-data/A/A/A/TRAAAAK128F9318786.json\n",
      "song-data/A/A/A/TRAAAAV128F421A322.json\n",
      "song-data/A/A/A/TRAAABD128F429CF47.json\n",
      "song-data/A/A/A/TRAAACN128F9355673.json\n"
     ]
    }
   ],
   "source": [
    "# STAGING TABLES\n",
    "\n",
    "## recall from dwh-3.cfg\n",
    "## first question: these are non-partitioned tables or partitioned tables?\n",
    "## these are non-partitioned tables or partitioned tables? Song data and Log data are partitioned tables, as instruction said\n",
    "\"\"\" \n",
    "LOG_DATA='s3://udacity-dend/log-data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song-data'\n",
    "\"\"\"\n",
    "\n",
    "## can we write to check whehther these are non-partitioned tables or partitioned tables?\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3',  # Changed from 'aws-s3' to 's3'\n",
    "                  aws_access_key_id=config['AWS']['KEY'],\n",
    "                  aws_secret_access_key=config['AWS']['SECRET'],\n",
    "                  region_name=config['AWS']['REGION'])\n",
    "\n",
    "# List objects in log-data bucket\n",
    "log_response = s3.list_objects_v2(Bucket='udacity-dend', Prefix='log-data/')\n",
    "song_response = s3.list_objects_v2(Bucket='udacity-dend', Prefix='song-data/')\n",
    "\n",
    "print(\"Log data structure:\")\n",
    "for obj in log_response.get('Contents', [])[:5]:\n",
    "    print(obj['Key'])\n",
    "\n",
    "print(\"\\nSong data structure:\") \n",
    "for obj in song_response.get('Contents', [])[:5]:\n",
    "    print(obj['Key'])\n",
    "\n",
    "# If we see prefixes like year/month/day or artist/year etc,\n",
    "# it indicates partitioned data\n",
    "# If files are flat in the bucket, likely non-partitioned\n",
    "\n",
    "## The log files in the dataset you'll be working with are partitioned by year and month\n",
    "## The song files are partitioned by the first three letters of each song's track ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONPath file content:\n",
      "{\n",
      "  \"jsonpaths\": [\n",
      "    \"$['artist']\",\n",
      "    \"$['auth']\",\n",
      "    \"$['firstName']\",\n",
      "    \"$['gender']\",\n",
      "    \"$['itemInSession']\",\n",
      "    \"$['lastName']\",\n",
      "    \"$['length']\",\n",
      "    \"$['level']\",\n",
      "    \"$['location']\",\n",
      "    \"$['method']\",\n",
      "    \"$['page']\",\n",
      "    \"$['registration']\",\n",
      "    \"$['sessionId']\",\n",
      "    \"$['song']\",\n",
      "    \"$['status']\",\n",
      "    \"$['ts']\",\n",
      "    \"$['userAgent']\",\n",
      "    \"$['userId']\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Log JSON Metadata\n",
    "\"\"\"\n",
    "The log_json_path.json file is used when loading JSON data into Redshift. It specifies the structure of the JSON data so that Redshift can properly parse and load it into the staging tables.\n",
    "\n",
    "In the context of this project, you will need the log_json_path.json file in the COPY command, which is responsible for loading the log data from S3 into the staging tables in Redshift. The log_json_path.json file tells Redshift how to interpret the JSON data and extract the relevant fields. This is essential for further processing and transforming the data into the desired analytics tables.\n",
    "\n",
    "Below is what data is in log_json_path.json.\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "# Get the content of the log_json_path.json file\n",
    "response = s3.get_object(Bucket='udacity-dend', Key='log_json_path.json')\n",
    "jsonpath_content = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "print(\"JSONPath file content:\")\n",
    "print(json.dumps(jsonpath_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song file content:\n",
      "{\n",
      "  \"song_id\": \"SOBLFFE12AF72AA5BA\",\n",
      "  \"num_songs\": 1,\n",
      "  \"title\": \"Scream\",\n",
      "  \"artist_name\": \"Adelitas Way\",\n",
      "  \"artist_latitude\": null,\n",
      "  \"year\": 2009,\n",
      "  \"duration\": 213.9424,\n",
      "  \"artist_id\": \"ARJNIUY12298900C91\",\n",
      "  \"artist_longitude\": null,\n",
      "  \"artist_location\": \"\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Example log file content (first record):\n",
      "{\n",
      "  \"artist\": null,\n",
      "  \"auth\": \"Logged In\",\n",
      "  \"firstName\": \"Theodore\",\n",
      "  \"gender\": \"M\",\n",
      "  \"itemInSession\": 0,\n",
      "  \"lastName\": \"Smith\",\n",
      "  \"length\": null,\n",
      "  \"level\": \"free\",\n",
      "  \"location\": \"Houston-The Woodlands-Sugar Land, TX\",\n",
      "  \"method\": \"GET\",\n",
      "  \"page\": \"Home\",\n",
      "  \"registration\": 1540306145796.0,\n",
      "  \"sessionId\": 154,\n",
      "  \"song\": null,\n",
      "  \"status\": 200,\n",
      "  \"ts\": 1541290555796,\n",
      "  \"userAgent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0\",\n",
      "  \"userId\": \"52\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get sample data from both datasets\n",
    "## Song and Log datasets\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id=config['AWS']['KEY'],\n",
    "                  aws_secret_access_key=config['AWS']['SECRET'],\n",
    "                  region_name=config['AWS']['REGION'])\n",
    "\n",
    "# 获取一个示例歌曲文件\n",
    "try:\n",
    "    song_response = s3.get_object(Bucket='udacity-dend', \n",
    "                                 Key='song-data/A/A/A/TRAAAAK128F9318786.json')\n",
    "    song_content = json.loads(song_response['Body'].read().decode('utf-8'))\n",
    "    print(\"Example song file content:\")\n",
    "    print(json.dumps(song_content, indent=2))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting song file: {e}\")\n",
    "\n",
    "# 获取一个示例日志文件\n",
    "try:\n",
    "    log_response = s3.get_object(Bucket='udacity-dend', \n",
    "                                Key='log-data/2018/11/2018-11-04-events.json')\n",
    "    # 读取原始内容并按行分割\n",
    "    log_content = log_response['Body'].read().decode('utf-8')\n",
    "    # 每行都是一个独立的 JSON 对象\n",
    "    log_records = [json.loads(line) for line in log_content.strip().split('\\n')]\n",
    "    \n",
    "    print(\"Example log file content (first record):\")\n",
    "    print(json.dumps(log_records[0], indent=2))  # 只显示第一条记录\n",
    "except Exception as e:\n",
    "    print(f\"Error getting log file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Write ETL queries for staging tables\n",
    "\n",
    "## Step 2.2: for Final tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully\n",
      "Executing query: \n",
      "COPY staging_events\n",
      "FROM 's3://udacity-dend/log-data'\n",
      "iam_role 'arn:aws:iam::339713039693:role/dwhR...\n",
      "Executing query: \n",
      "COPY staging_songs\n",
      "FROM 's3://udacity-dend/song-data'\n",
      "iam_role 'arn:aws:iam::339713039693:role/dwhR...\n",
      "Error loading staging tables: Load into table 'staging_songs' failed.  Check 'stl_load_errors' system table for details.\n",
      "\n",
      "Executing query: \n",
      "INSERT INTO songplays (START_TIME, USER_ID, LEVEL, SONG_ID, ARTIST_ID, SESSION_ID, LOCATION, USER_A...\n",
      "Executing query: \n",
      "INSERT INTO users\n",
      "SELECT DISTINCT userId, firstName, lastName, gender, level\n",
      "FROM staging_events\n",
      "WH...\n",
      "Executing query: \n",
      "INSERT INTO songs\n",
      "SELECT\n",
      "    DISTINCT song_id, title, artist_id, year, duration\n",
      "FROM staging_songs\n",
      "...\n",
      "Executing query: \n",
      "INSERT INTO artists\n",
      "SELECT\n",
      "    DISTINCT artist_id, artist_name, artist_location, artist_latitude, a...\n",
      "Executing query: \n",
      "insert into time\n",
      "SELECT DISTINCT\n",
      "       TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second' as star...\n",
      "Analytics tables loaded successfully:\n",
      "songplays: 0 rows\n",
      "users: 104 rows\n",
      "songs: 0 rows\n",
      "artists: 0 rows\n",
      "time: 8023 rows\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    try:\n",
    "        for query in copy_table_queries:\n",
    "            print(f\"Executing query: {query[:100]}...\")  # Print first 100 chars for debugging\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "        # Validate staging data\n",
    "        cur.execute(\"SELECT COUNT(*) FROM staging_events\")\n",
    "        events_count = cur.fetchone()[0]\n",
    "        cur.execute(\"SELECT COUNT(*) FROM staging_songs\")\n",
    "        songs_count = cur.fetchone()[0]\n",
    "        print(f\"Staging tables loaded successfully:\")\n",
    "        print(f\"staging_events: {events_count} rows\")\n",
    "        print(f\"staging_songs: {songs_count} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading staging tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    try:\n",
    "        for query in insert_table_queries:\n",
    "            print(f\"Executing query: {query[:100]}...\")  # Print first 100 chars for debugging\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "        # Validate analytics tables\n",
    "        validation_queries = [\n",
    "            (\"songplays\", \"SELECT COUNT(*) FROM songplays\"),\n",
    "            (\"users\", \"SELECT COUNT(*) FROM users\"),\n",
    "            (\"songs\", \"SELECT COUNT(*) FROM songs\"),\n",
    "            (\"artists\", \"SELECT COUNT(*) FROM artists\"),\n",
    "            (\"time\", \"SELECT COUNT(*) FROM time\")\n",
    "        ]\n",
    "        \n",
    "        print(\"Analytics tables loaded successfully:\")\n",
    "        for table, query in validation_queries:\n",
    "            cur.execute(query)\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"{table}: {count} rows\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting into tables: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Main ETL execution\n",
    "conn = None\n",
    "cur = None\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    print(\"Connected to Redshift successfully\")\n",
    "    \n",
    "    # Execute ETL process\n",
    "    load_staging_tables(cur, conn)\n",
    "    insert_tables(cur, conn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during ETL process: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT COUNT(*) FROM staging_events\n",
      "[8056]\n",
      "\n",
      "    SELECT COUNT(*) FROM staging_songs\n",
      "[0]\n",
      "\n",
      "    SELECT COUNT(*) FROM songplays\n",
      "[0]\n",
      "\n",
      "    SELECT COUNT(*) FROM users\n",
      "[104]\n",
      "\n",
      "    SELECT COUNT(*) FROM songs\n",
      "[0]\n",
      "\n",
      "    SELECT COUNT(*) FROM artists\n",
      "[0]\n",
      "\n",
      "    SELECT COUNT(*) FROM time\n",
      "[8023]\n"
     ]
    }
   ],
   "source": [
    "# Check row counts for all tables\n",
    "table_queries = [\n",
    "    \"SELECT COUNT(*) FROM staging_events\",\n",
    "    \"SELECT COUNT(*) FROM staging_songs\",\n",
    "    \"SELECT COUNT(*) FROM songplays\",\n",
    "    \"SELECT COUNT(*) FROM users\",\n",
    "    \"SELECT COUNT(*) FROM songs\",\n",
    "    \"SELECT COUNT(*) FROM artists\",\n",
    "    \"SELECT COUNT(*) FROM time\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Connect to Redshift cluster\n",
    "    conn = psycopg2.connect(\n",
    "        host=config['CLUSTER']['HOST'],\n",
    "        dbname=config['CLUSTER']['DB_NAME'],\n",
    "        user=config['CLUSTER']['DB_USER'],\n",
    "        password=config['CLUSTER']['DB_PASSWORD'],\n",
    "        port=int(config['CLUSTER']['DB_PORT'])\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Execute each count query and print results\n",
    "    for query in table_queries:\n",
    "        print(f\"\\n    {query}\")\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "        print(f\"[{result[0]}]\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking row counts: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cur is not None:\n",
    "        cur.close()\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Row Counts:\n",
      "--------------------------------------------------\n",
      "Staging Events       |      8,056 rows\n",
      "Staging Songs        |          0 rows\n",
      "Songplays (Fact)     |          0 rows\n",
      "Users (Dim)          |        104 rows\n",
      "Songs (Dim)          |          0 rows\n",
      "Artists (Dim)        |          0 rows\n",
      "Time (Dim)           |      8,023 rows\n",
      "--------------------------------------------------\n",
      "\n",
      "Data Quality Checks:\n",
      "--------------------------------------------------\n",
      "Null songplay_id in songplays  |          0 rows\n",
      "Error during validation: column \"user_id\" does not exist in users\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simplified the code by removing the non-null checks and focusing only on primary key validation since that's the most critical aspect\n",
    "\n",
    "def check_table_counts():\n",
    "    # Dictionary of tables and their corresponding queries\n",
    "    table_counts = {\n",
    "        'Staging Events': \"SELECT COUNT(*) FROM staging_events\",\n",
    "        'Staging Songs': \"SELECT COUNT(*) FROM staging_songs\",\n",
    "        'Songplays (Fact)': \"SELECT COUNT(*) FROM songplays\",\n",
    "        'Users (Dim)': \"SELECT COUNT(*) FROM users\",\n",
    "        'Songs (Dim)': \"SELECT COUNT(*) FROM songs\",\n",
    "        'Artists (Dim)': \"SELECT COUNT(*) FROM artists\",\n",
    "        'Time (Dim)': \"SELECT COUNT(*) FROM time\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Connect to Redshift cluster\n",
    "        conn = psycopg2.connect(\n",
    "            host=config['CLUSTER']['HOST'],\n",
    "            dbname=config['CLUSTER']['DB_NAME'],\n",
    "            user=config['CLUSTER']['DB_USER'],\n",
    "            password=config['CLUSTER']['DB_PASSWORD'],\n",
    "            port=int(config['CLUSTER']['DB_PORT'])\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        print(\"\\nTable Row Counts:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Execute each count query and print results\n",
    "        for table_name, query in table_counts.items():\n",
    "            cur.execute(query)\n",
    "            count = cur.fetchone()[0]\n",
    "            print(f\"{table_name:<20} | {count:>10,} rows\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Additional validation queries\n",
    "        print(\"\\nData Quality Checks:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Check for null values in primary keys\n",
    "        null_checks = {\n",
    "            'songplays': 'songplay_id',\n",
    "            'users': 'user_id',\n",
    "            'songs': 'song_id',\n",
    "            'artists': 'artist_id',\n",
    "            'time': 'start_time'\n",
    "        }\n",
    "        \n",
    "        for table, pk in null_checks.items():\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {table} WHERE {pk} IS NULL\")\n",
    "            null_count = cur.fetchone()[0]\n",
    "            print(f\"Null {pk} in {table:<10} | {null_count:>10} rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during validation: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "# Run the validation\n",
    "check_table_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not match\n",
    "\n",
    "Instruction expected\n",
    "```\n",
    "    SELECT COUNT(*) FROM staging_events\n",
    "[8056]\n",
    "\n",
    "    SELECT COUNT(*) FROM staging_songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM songplays\n",
    "[6820]\n",
    "\n",
    "    SELECT COUNT(*) FROM users\n",
    "[104]\n",
    "\n",
    "    SELECT COUNT(*) FROM songs\n",
    "[14896]\n",
    "\n",
    "    SELECT COUNT(*) FROM artists\n",
    "[10025]\n",
    "\n",
    "    SELECT COUNT(*) FROM time\n",
    "[6813]\n",
    "```\n",
    "\n",
    "but we got\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "Staging Events       |      8,056 rows\n",
    "Staging Songs        |    385,212 rows\n",
    "Songplays (Fact)     |      7,267 rows\n",
    "Users (Dim)          |        105 rows\n",
    "Songs (Dim)          |    384,955 rows\n",
    "Artists (Dim)        |     45,262 rows\n",
    "Time (Dim)           |      8,023 rows\n",
    "--------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Discrepancy Investigation\n",
      "==================================================\n",
      "\n",
      "Staging Songs Data Range\n",
      "------------------------\n",
      "first_song last_song  unique_songs  total_rows\n",
      "      None      None             0           0\n",
      "\n",
      "\n",
      "\n",
      "Duplicate Songs Check\n",
      "---------------------\n",
      "Empty DataFrame\n",
      "Columns: [song_id, occurrence_count]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "Songplays Join Analysis\n",
      "-----------------------\n",
      " total_matches  unique_songs_matched  unique_artists_matched\n",
      "             0                     0                       0\n",
      "\n",
      "\n",
      "\n",
      "Artist Duplicates\n",
      "-----------------\n",
      "Empty DataFrame\n",
      "Columns: [artist_id, artist_name, occurrence_count]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "Time Table Verification\n",
      "-----------------------\n",
      " unique_timestamps  total_timestamps\n",
      "              6813              6820\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def investigate_data_discrepancies():\n",
    "    diagnostic_queries = {\n",
    "        \"Staging Songs Data Range\": \"\"\"\n",
    "            SELECT \n",
    "                MIN(song_id) as first_song,\n",
    "                MAX(song_id) as last_song,\n",
    "                COUNT(DISTINCT song_id) as unique_songs,\n",
    "                COUNT(*) as total_rows\n",
    "            FROM staging_songs;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Duplicate Songs Check\": \"\"\"\n",
    "            SELECT \n",
    "                song_id, \n",
    "                COUNT(*) as occurrence_count\n",
    "            FROM staging_songs\n",
    "            GROUP BY song_id\n",
    "            HAVING COUNT(*) > 1\n",
    "            ORDER BY occurrence_count DESC\n",
    "            LIMIT 5;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Songplays Join Analysis\": \"\"\"\n",
    "            WITH matching_songs AS (\n",
    "                SELECT se.song, se.artist, ss.song_id, ss.artist_id\n",
    "                FROM staging_events se\n",
    "                JOIN staging_songs ss\n",
    "                ON se.song = ss.title AND se.artist = ss.artist_name\n",
    "                WHERE se.page = 'NextSong'\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(*) as total_matches,\n",
    "                COUNT(DISTINCT song_id) as unique_songs_matched,\n",
    "                COUNT(DISTINCT artist_id) as unique_artists_matched\n",
    "            FROM matching_songs;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Artist Duplicates\": \"\"\"\n",
    "            SELECT \n",
    "                artist_id,\n",
    "                artist_name,\n",
    "                COUNT(*) as occurrence_count\n",
    "            FROM staging_songs\n",
    "            GROUP BY artist_id, artist_name\n",
    "            HAVING COUNT(*) > 1\n",
    "            ORDER BY occurrence_count DESC\n",
    "            LIMIT 5;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Time Table Verification\": \"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second') as unique_timestamps,\n",
    "                COUNT(*) as total_timestamps\n",
    "            FROM staging_events\n",
    "            WHERE page = 'NextSong';\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Connect to Redshift\n",
    "        conn = psycopg2.connect(\n",
    "            host=config['CLUSTER']['HOST'],\n",
    "            dbname=config['CLUSTER']['DB_NAME'],\n",
    "            user=config['CLUSTER']['DB_USER'],\n",
    "            password=config['CLUSTER']['DB_PASSWORD'],\n",
    "            port=int(config['CLUSTER']['DB_PORT'])\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        print(\"\\nData Discrepancy Investigation\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for title, query in diagnostic_queries.items():\n",
    "            print(f\"\\n{title}\")\n",
    "            print(\"-\" * len(title))\n",
    "            \n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            # Get column names\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            \n",
    "            # Create DataFrame for prettier display\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            print(df.to_string(index=False))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running diagnostic queries: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "# Run the diagnostic queries\n",
    "investigate_data_discrepancies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Provide example queries and results for song play analysis. We do not provide you any of these. You, as part of the Data Engineering team were tasked to build this ETL. Thorough study has gone into the star schema, tables, and columns required. The ETL will be effective and provide the data and in the format required. However, as an exercise, it seems almost silly to NOT show SOME examples of potential queries that could be ran by the users. PLEASE use your imagination here. For example, what is the most played song? When is the highest usage time of day by hour for songs? It would not take much to imagine what types of questions that corporate users of the system would find interesting. Including those queries and the answers makes your project far more compelling when using it as an example of your work to people / companies that would be interested. You could simply have a section of sql_queries.py that is executed after the load is done that prints a question and then the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkify Music Streaming Analytics\n",
      "==================================================\n",
      "\n",
      "Most Popular Songs\n",
      "------------------\n",
      "Empty DataFrame\n",
      "Columns: [title, artist, play_count]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "Peak Usage Hours\n",
      "----------------\n",
      "Empty DataFrame\n",
      "Columns: [hour, plays]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "User Activity by Level\n",
      "----------------------\n",
      "Empty DataFrame\n",
      "Columns: [level, user_count, total_plays, avg_plays_per_user]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "Most Active Users\n",
      "-----------------\n",
      "Error running analytical queries: column u.user_id does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_analytical_queries():\n",
    "    # Dictionary of analytical queries with their descriptions\n",
    "    analytical_queries = {\n",
    "        \"Most Popular Songs\": \"\"\"\n",
    "            SELECT s.title, a.name as artist, COUNT(*) as play_count\n",
    "            FROM songplays sp\n",
    "            JOIN songs s ON sp.song_id = s.song_id\n",
    "            JOIN artists a ON sp.artist_id = a.artist_id\n",
    "            GROUP BY s.title, a.name\n",
    "            ORDER BY play_count DESC\n",
    "            LIMIT 5;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Peak Usage Hours\": \"\"\"\n",
    "            SELECT t.hour, COUNT(*) as plays\n",
    "            FROM songplays sp\n",
    "            JOIN time t ON sp.start_time = t.start_time\n",
    "            GROUP BY t.hour\n",
    "            ORDER BY plays DESC\n",
    "            LIMIT 24;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"User Activity by Level\": \"\"\"\n",
    "            SELECT level, COUNT(DISTINCT user_id) as user_count,\n",
    "                   COUNT(*) as total_plays,\n",
    "                   ROUND(COUNT(*) / COUNT(DISTINCT user_id)::float, 2) as avg_plays_per_user\n",
    "            FROM songplays\n",
    "            GROUP BY level;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Most Active Users\": \"\"\"\n",
    "            SELECT u.first_name, u.last_name, u.level,\n",
    "                   COUNT(*) as play_count\n",
    "            FROM songplays sp\n",
    "            JOIN users u ON sp.user_id = u.user_id\n",
    "            GROUP BY u.user_id, u.first_name, u.last_name, u.level\n",
    "            ORDER BY play_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Popular Music by Day of Week\": \"\"\"\n",
    "            SELECT t.weekday,\n",
    "                   COUNT(*) as total_plays,\n",
    "                   COUNT(DISTINCT sp.user_id) as unique_users\n",
    "            FROM songplays sp\n",
    "            JOIN time t ON sp.start_time = t.start_time\n",
    "            GROUP BY t.weekday\n",
    "            ORDER BY t.weekday;\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Connect to Redshift\n",
    "        conn = psycopg2.connect(\n",
    "            host=config['CLUSTER']['HOST'],\n",
    "            dbname=config['CLUSTER']['DB_NAME'],\n",
    "            user=config['CLUSTER']['DB_USER'],\n",
    "            password=config['CLUSTER']['DB_PASSWORD'],\n",
    "            port=int(config['CLUSTER']['DB_PORT'])\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        print(\"\\nSparkify Music Streaming Analytics\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Execute each analytical query and display results\n",
    "        for title, query in analytical_queries.items():\n",
    "            print(f\"\\n{title}\")\n",
    "            print(\"-\" * len(title))\n",
    "            \n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            # Get column names from cursor description\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            \n",
    "            # Create DataFrame for prettier display\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            print(df.to_string(index=False))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running analytical queries: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "# Run the analytical queries\n",
    "run_analytical_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step: Project Rubric requirements\n",
    "\n",
    "## Readme.md\n",
    "\n",
    "The project shows proper use of documentation.\n",
    "\n",
    "The README file includes a summary of the project, how to run the Python scripts, and an explanation of the files in the repository. Comments are used effectively and each function has a docstring.\n",
    "\n",
    "\n",
    "## Clean modular Code\n",
    "\n",
    "Scripts have an intuitive, easy-to-follow structure with code separated into logical functions. Naming for variables and functions follows the PEP8 style guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Good practice\n",
    "\n",
    "## Create and Delete Cluster\n",
    "\n",
    "## IAM roles\n",
    "\n",
    "## Access Key: Create and Delete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
